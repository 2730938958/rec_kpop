from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from neo4j import GraphDatabase  # å¯¼å…¥Neo4jé©±åŠ¨
import torch
import asyncio
from langchain_community.utilities import SerpAPIWrapper
import os
os.environ['SERPAPI_API_KEY'] = '5f637d55472a8b1a905c0648dd0b79637288ca2e28c5a35bd248c38b7d921ceb'
from transformers import GenerationConfig
# åœ¨ flask_test.py ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆå¯æ”¾åœ¨æ¨¡å‹åŠ è½½éƒ¨åˆ†ä¹‹åï¼‰

# å¯¼å…¥å¿…è¦çš„ LangChain ç»„ä»¶
from langchain.tools import Tool
from langchain.agents import AgentExecutor, create_react_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.memory import ConversationBufferMemory
from langchain import PromptTemplate
import datetime

# ä» langchain_qwen_agent_test.py å¯¼å…¥ Qwen25LLM ç±»
from langchain_qwen_agent_test import Qwen25LLM

# åˆå§‹åŒ– LangChain å·¥å…·
def get_current_time(*args, **kwargs) -> str:
    """è·å–å½“å‰ç³»ç»Ÿæ—¶é—´"""
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return f"current time: {current_time}"

time_tool = Tool(
    name="GetCurrentTime",
    func=get_current_time,
    description="è·å–å½“å‰ç³»ç»Ÿæ—¶é—´ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD HH:MM:SSï¼Œè°ƒç”¨æ—¶Action Inputåº”ä¸ºç©º"
)

# åˆå§‹åŒ–æœç´¢å·¥å…·å’ŒLLM
tavily_search = TavilySearchResults(max_results=2)


# å®šä¹‰Agentæç¤ºæ¨¡æ¿
agent_prompt_template = """
the history conversation:

{chat_history}

Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Note: If sufficient information has not been obtained after three tool calls, please provide the best answer directly based on the available information.
Begin!

Question: {input}
Thought:{agent_scratchpad}
"""

# åˆ›å»ºæç¤ºæ¨¡æ¿
agent_prompt = PromptTemplate(
    input_variables=["chat_history", "tools", "tool_names", "input", "agent_scratchpad"],
    template=agent_prompt_template
)

# åˆå§‹åŒ–å¯¹è¯è®°å¿†ï¼ˆä½¿ç”¨å…¨å±€è®°å¿†å­˜å‚¨å¯¹è¯å†å²ï¼‰
agent_memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# å®šä¹‰å·¥å…·åˆ—è¡¨
tools = [tavily_search, time_tool]

# ---------------------- 1. åˆå§‹åŒ–FastAPIå’ŒCORSé…ç½® ----------------------
app = FastAPI(title="Qwen LLM + Neo4j Service")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒæ›¿æ¢ä¸ºå…·ä½“å‰ç«¯åŸŸå
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ---------------------- 2. å®šä¹‰è¯·æ±‚æ¨¡å‹ ----------------------
class LLMRequest(BaseModel):
    prompt: str  # ç”¨æˆ·è¾“å…¥ï¼ˆå«â€œçŸ¥è¯†åº“â€åˆ™è§¦å‘Neo4jï¼‰
    model: str = "Qwen/Qwen2.5-7B-Instruct"
    temperature: float = 0.7
    max_new_tokens: int = 512


# ---------------------- 3. Neo4jé…ç½®ä¸æ£€ç´¢å‡½æ•°ï¼ˆæ–°å¢ï¼‰ ----------------------
# Neo4jè¿æ¥ä¿¡æ¯ï¼ˆæ›¿æ¢ä¸ºä½ çš„å®é™…é…ç½®ï¼‰
NEO4J_URI = "neo4j+s://30acc171.databases.neo4j.io"
NEO4J_AUTH = ("neo4j", "A_Arjzc6q8TkRAC0wtSULmanpNpTLSmCJqtXmNrtyMY")
NEO4J_DATABASE = "neo4j"

def load_singer_list(file_path):
    singer_set = set()  # ç”¨setæ¯”listå¿«ï¼Œé¿å…é‡å¤åŒ¹é…
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            singer = line.strip()  # å»é™¤æ¯è¡Œçš„æ¢è¡Œç¬¦ã€ç©ºæ ¼
            if singer:  # è·³è¿‡ç©ºè¡Œ
                singer_set.add(singer.lower())  # è½¬å°å†™ï¼Œå®ç°ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
    return singer_set

def detect_singer(input_text, singer_set):
    input_text_lower = input_text.lower()  # è¾“å…¥æ–‡æœ¬ä¹Ÿè½¬å°å†™
    detected_singers = []
    for singer in singer_set:
        if singer in input_text_lower:  # å­—ç¬¦ä¸²åŒ…å«åŒ¹é…ï¼ˆå¦‚â€œæˆ‘å–œæ¬¢itzyâ€ä¼šåŒ¹é…åˆ°â€œitzyâ€ï¼‰
            detected_singers.append(singer.title())  # è½¬å›é¦–å­—æ¯å¤§å†™ï¼Œè¾“å‡ºæ ‡å‡†å
    return detected_singers

def retrieve_from_neo4j(artist_name: str = "ITZY") -> list[dict]:
    """
    ä»Neo4jæ£€ç´¢ç›¸å…³èŠ‚ç‚¹æ•°æ®ï¼ˆä»…å½“ç”¨æˆ·è¾“å…¥å«â€œçŸ¥è¯†åº“â€æ—¶è°ƒç”¨ï¼‰
    :param artist_name: æ£€ç´¢çš„ç›®æ ‡è‰ºäººï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
    :return: ç›¸å…³èŠ‚ç‚¹çš„KVåˆ—è¡¨ï¼ˆç©ºåˆ—è¡¨è¡¨ç¤ºæ£€ç´¢å¤±è´¥/æ— æ•°æ®ï¼‰
    """
    driver = None
    related_li = []
    try:
        # å»ºç«‹Neo4jè¿æ¥
        driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)
        driver.verify_connectivity()  # éªŒè¯è¿æ¥æœ‰æ•ˆæ€§
        print("âœ… Neo4jè¿æ¥æˆåŠŸï¼Œå¼€å§‹æ£€ç´¢æ•°æ®")

        # æ‰§è¡ŒCypheræŸ¥è¯¢ï¼ˆåŒ¹é…ç›®æ ‡è‰ºäººçš„ç›¸å…³èŠ‚ç‚¹ï¼Œå–å‰5æ¡é¿å…æ•°æ®è¿‡è½½ï¼‰
        records, summary, keys = driver.execute_query("""
            MATCH (ITZY:Artist {artistName: "ITZY"})-[r]-(related)
            RETURN ITZY, r, related
            """,
            database_="neo4j".replace('ITZY', artist_name),)

        related_li = []
        # Loop through results and do something with them
        for record in records[:5]:
            related_li.append({
                'artistName': record.data()['related']['artistName'],
                'topTrack': record.data()['related']['topTrack'],
                'topTrackLink': record.data()['related']['topTrackLink'],
                'topTrackAlbum': record.data()['related']['topTrackAlbum'],
                'topTrackAlbumLink': record.data()['related']['topTrackAlbumLink'],
            })  # obtain record as dict

        print(f"âœ… Neo4jæ£€ç´¢å®Œæˆï¼Œè·å–åˆ° {len(related_li)} ä¸ªç›¸å…³èŠ‚ç‚¹")
        return related_li

    except Exception as e:
        print(f"âŒ Neo4jæ£€ç´¢å¤±è´¥: {str(e)}")
        return []  # æ£€ç´¢å¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸é˜»æ–­LLMæµç¨‹

    finally:
        # ç¡®ä¿å…³é—­è¿æ¥ï¼Œé¿å…èµ„æºæ³„æ¼
        if driver:
            driver.close()
            print("âœ… Neo4jè¿æ¥å·²å…³é—­")


# ---------------------- 4. LLMæ¨¡å‹åŠ è½½ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰ ----------------------
model_name = "Qwen/Qwen2.5-7B-Instruct"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
singer_list = load_singer_list("singer.txt")
print("å¼€å§‹åŠ è½½Qwenæ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
generation_config = GenerationConfig(
            max_new_tokens=512,
            do_sample=False,
            temperature=0.7,
            top_p=0.95,
            eos_token_id=151645,
            pad_token_id=151643
        )
langchain_llm = Qwen25LLM(model=model, tokenizer=tokenizer, generation_config=generation_config)  # å®ä¾‹åŒ–Qwen25LLM
# åˆ›å»ºAgentæ‰§è¡Œå™¨
agent_executor = AgentExecutor(
    agent=create_react_agent(langchain_llm, tools, agent_prompt),
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,
    memory=agent_memory,
    max_iterations=3,
    early_stopping_method="force"
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {model.device}")


# ---------------------- 5. æ–‡æœ¬ç”Ÿæˆå‡½æ•°ï¼ˆæ–°å¢Neo4jæ•°æ®æ•´åˆé€»è¾‘ï¼‰ ----------------------
def generate_text(text: str, temperature: float, max_new_tokens: int) -> str:
    """åŸæœ‰æ–‡æœ¬ç”Ÿæˆé€»è¾‘ï¼Œä¿æŒä¸å˜"""
    model_inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(model.device)
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=max_new_tokens,
        do_sample=temperature > 0,
        temperature=temperature,
        num_return_sequences=1,
        pad_token_id=tokenizer.pad_token_id  # é¿å…è­¦å‘Š
    )
    generated_ids_slice = generated_ids[0][len(model_inputs.input_ids[0]):]
    return tokenizer.decode(generated_ids_slice, skip_special_tokens=True)


def build_prompt_with_neo4j(user_prompt: str, neo4j_data: list[dict], singer) -> str:
    """
    æ•´åˆNeo4jæ•°æ®æ„å»ºæ–°Promptï¼ˆä»…å½“æœ‰Neo4jæ•°æ®æ—¶è°ƒç”¨ï¼‰
    :param user_prompt: ç”¨æˆ·åŸå§‹è¾“å…¥
    :param neo4j_data: Neo4jæ£€ç´¢åˆ°çš„KVåˆ—è¡¨
    :return: åŒ…å«çŸ¥è¯†åº“ä¿¡æ¯çš„å®Œæ•´Prompt
    """
    # æ ¼å¼åŒ–Neo4jæ•°æ®ä¸ºå¯è¯»æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€ä¸ªKVï¼‰
    neo4j_info = ""
    for idx, node in enumerate(neo4j_data, 1):
        neo4j_info += f"\nã€è‰ºäºº{idx}ã€‘\n"
        for key, value in node.items():
            neo4j_info += f"- {key}ï¼š{value}\n"

    # ç»„è£…å«çŸ¥è¯†åº“çš„Prompt
    print(neo4j_info)
    full_prompt = f"""
    ä½ æ˜¯ä¸“ä¸šçš„K-PopéŸ³ä¹åŠ©æ‰‹ï¼Œç”¨æˆ·æƒ³è®©ä½ æ¨èå‡ ä¸ªkpopæ­Œæ‰‹ï¼Œä½ éœ€åŸºäºä»¥ä¸‹ã€çŸ¥è¯†åº“æ­Œæ‰‹ä¿¡æ¯ã€‘è¿›è¡Œæ¨èï¼Œè¦æ±‚å›ç­”è‡ªç„¶ï¼Œç”¨è‹±è¯­å›ç­”ï¼š
    
    ã€çŸ¥è¯†åº“æ­Œæ‰‹ä¿¡æ¯ã€‘
    {neo4j_info}

    
    """
    return full_prompt  # å»é™¤å¤šä½™ç©ºæ ¼


# ---------------------- 6. APIç«¯ç‚¹ï¼ˆæ ¸å¿ƒï¼šåˆ¤æ–­æ˜¯å¦è§¦å‘Neo4jï¼‰ ----------------------
@app.post("/api/llm")
async def generate_response(request: LLMRequest):
    try:
        user_prompt = request.prompt.strip()
        neo4j_data = []  # å­˜å‚¨Neo4jæ•°æ®ï¼ˆé»˜è®¤ç©ºï¼‰
        final_prompt = user_prompt  # æœ€ç»ˆä¼ å…¥LLMçš„Promptï¼ˆé»˜è®¤ç”¨æˆ·åŸå§‹è¾“å…¥ï¼‰

        # ---------------------- å…³é”®åˆ¤æ–­ï¼šç”¨æˆ·è¾“å…¥æ˜¯å¦å«â€œçŸ¥è¯†åº“â€ ----------------------
        if "knowledge base" in user_prompt:
            print("ğŸ” æ£€æµ‹åˆ°ç”¨æˆ·è¾“å…¥å«â€œçŸ¥è¯†åº“â€ï¼Œè°ƒç”¨Neo4jæ£€ç´¢")
            singer = detect_singer(user_prompt, singer_list)
            singer = singer[0] if len(singer) > 0 else "ITZY"
            neo4j_data = retrieve_from_neo4j(artist_name=singer)

            if neo4j_data:
                final_prompt = build_prompt_with_neo4j(user_prompt, neo4j_data, singer)
            else:
                final_prompt = f"{user_prompt}\nï¼ˆæ³¨ï¼šçŸ¥è¯†åº“æš‚æœªè·å–åˆ°ç›¸å…³æ•°æ®ï¼Œå°†åŸºäºé»˜è®¤çŸ¥è¯†å›ç­”ï¼‰"

        elif "search" in user_prompt or "internet" in user_prompt:
            search = SerpAPIWrapper()
            result = search.run(user_prompt)
            final_prompt = f"ç”¨æˆ·ç°åœ¨æœ‰ä»¥ä¸‹è”ç½‘æœç´¢è¦æ±‚[{user_prompt}]\nä»¥ä¸‹æ˜¯è”ç½‘æœç´¢åˆ°çš„å†…å®¹ï¼š{result},è¯·ç›´æ¥ç”Ÿæˆä¸€æ®µç»™ç”¨æˆ·çš„ç­”æ¡ˆ"

        else:
            print("ä½¿ç”¨LangChain Agentå›ç­”ing")
            # è°ƒç”¨LangChain Agentå¤„ç†ç”¨æˆ·è¾“å…¥
            loop = asyncio.get_event_loop()
            # ç”±äºAgentè°ƒç”¨æ˜¯åŒæ­¥çš„ï¼Œéœ€è¦ç”¨run_in_executoråŒ…è£…ä¸ºå¼‚æ­¥
            agent_result = await loop.run_in_executor(
                None,
                lambda: agent_executor.invoke({"input": user_prompt})
            )
            response_text = agent_result["output"]
            # ç›´æ¥è¿”å›Agentç»“æœï¼Œä¸éœ€è¦å†è°ƒç”¨generate_text
            return {
                "response": response_text,
                "recommendations": neo4j_data,
                "used_knowledge_base": False,
                "knowledge_base_count": 0
            }

        # ---------------------- æ„å»ºå¯¹è¯æ¨¡æ¿å¹¶è°ƒç”¨LLMï¼ˆåŸæœ‰é€»è¾‘ï¼Œå¤„ç†çŸ¥è¯†åº“å’Œæœç´¢åˆ†æ”¯ï¼‰ ----------------------
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": final_prompt}
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(
            None,
            generate_text,
            text,
            request.temperature,
            request.max_new_tokens
        )
        print(f'ğŸ“ æ¨¡å‹ç”Ÿæˆçš„ç­”å¤ï¼š{response_text}')

        return {
            "response": response_text,
            "recommendations": neo4j_data,
            "used_knowledge_base": "knowledge base" in user_prompt,
            "knowledge_base_count": len(neo4j_data)
        }

    except Exception as e:
        error_msg = f"æœåŠ¡å¤„ç†å¤±è´¥: {str(e)}"
        print(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)


# ---------------------- 7. æ ¹è·¯å¾„æµ‹è¯• ----------------------
@app.get("/")
async def root():
    return {
        "message": "Qwen LLM + Neo4j Service is running",
        "tip": "ç”¨æˆ·è¾“å…¥å«â€œçŸ¥è¯†åº“â€å…³é”®è¯æ—¶ï¼Œå°†è°ƒç”¨Neo4jæ£€ç´¢æ•°æ®"
    }


# ---------------------- 8. å¯åŠ¨æœåŠ¡ ----------------------
if __name__ == "__main__":
    import uvicorn

    # å•è¿›ç¨‹æ¨¡å¼ï¼ˆç¡®ä¿è°ƒè¯•æ–­ç‚¹ç”Ÿæ•ˆï¼‰
    uvicorn.run(app, host="0.0.0.0", port=7899, workers=1, reload=False)